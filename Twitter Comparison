library(rtweet) #For Twitter API
library(ggplot2) #For ploting
library(Rmisc) #For multiplot function
library(stringr) #For word cloud transformation
library(tm) #NLP 
library(wordcloud) #Wordcloud visualization

#Pull data from twitter

#Follow steps 1-7 in https://medium.com/@GalarnykMichael/accessing-data-from-twitter-api-using-r-part1-b387a1c7d3e

appname <- "APPNAME"
key <- "xxxxx"
secret <- "xxxxxxxxxxxxxxxxxxxxxx"

twitter_token <- create_token(
  app = appname,
  consumer_key = key,
  consumer_secret = secret)
 
 
#Pull comparison data out of twitter (1000 most recent tweets from twitterhandle)
 
c1 <- get_timelines("twitterhandle1", n = 1000,token = twitter_token)
c2 <- get_timelines("twitterhandle2", n = 1000,token = twitter_token)
c3 <- get_timelines("twitterhandle3", n = 1000,token = twitter_token)
c4 <- get_timelines("twitterhandle4", n = 1000,token = twitter_token)

#Create Histograms#

#Transform data for plots

tweet1 <- data.frame(Likes = c1$favorite_count)
tweet2 <- data.frame(Likes = c2$favorite_count)
tweet3 <- data.frame(Likes = c3$favorite_count)
tweet4 <- data.frame(Likes = c4$favorite_count)


# Add names for graph labels

tweet1$name <- 'l1'
tweet2$name <- 'l2'
tweet3$name <- 'l3'
tweet4$name <- 'l4'

# Save 4 histogram plots with average likes

p1 <- ggplot(tweet1, aes(x = Likes, fill = name)) +
  geom_histogram(breaks=c(seq(0,50,by=1), max(tweet1$Likes)),position = "identity") +
  coord_cartesian(xlim=c(0,50)) +
  geom_vline(aes(xintercept=mean(tweet1$Likes)), color="blue", linetype="dashed", size=1)+
  labs(caption = paste("Average Engagement:",round(mean(tweet1$Likes),2)))

p2 <- ggplot(tweet2, aes(x = Likes, fill = name)) +
  geom_histogram(breaks=c(seq(0,50,by=1), max(tweet1$Likes)),position = "identity") +
  coord_cartesian(xlim=c(0,50)) +
  geom_vline(aes(xintercept=mean(tweet2$Likes)), color="blue", linetype="dashed", size=1)+
  labs(caption = paste("Average Engagement:",round(mean(tweet2$Likes),2)))

p3 <- ggplot(tweet3, aes(x = Likes, fill = name)) +
  geom_histogram(breaks=c(seq(0,50,by=1), max(tweet1$Likes)),position = "identity") +
  coord_cartesian(xlim=c(0,50)) +
  geom_vline(aes(xintercept=mean(tweet3$Likes)), color="blue", linetype="dashed", size=1)+
  labs(caption = paste("Average Engagement:",round(mean(tweet3$Likes),2)))

p4 <- ggplot(tweet4, aes(x = Likes, fill = name)) +
  geom_histogram(breaks=c(seq(0,50,by=1), max(tweet1$Likes)),position = "identity") +
  coord_cartesian(xlim=c(0,50)) +
  geom_vline(aes(xintercept=mean(tweet4$Likes)), color="blue", linetype="dashed", size=1)+
  labs(caption = paste("Average Engagement:",round(mean(tweet4$Likes),2)))

multiplot(p1,p2,p3,p4,cols=2)


#Create word map#

#Clean data and transform for word cloud
##Add loop

tweet=c1$text
tweet_list=lapply(tweet, function(x) iconv(x, "latin1", "ASCII", sub=""))
tweet <- gsub(" ?(f|ht)tp(s?)://(.*)[.][a-z]+", " ", tweet) #remove url
tweet <- gsub("@.*?\\s", " ", tweet) #remove @
tweet = gsub('[[:punct:]]', ' ', tweet) #remove punct
tweet = gsub('[[:cntrl:]]', ' ', tweet) #remove control characters
tweet = gsub("amp", ' ', tweet) #remove control characters
tweet = gsub("  "," ",tweet) #Change dounle spaces to single spaces
tweet=unlist(tweet)
c1$textclean <- tweet

c1$tweet <- str_replace_all(c1$textclean, "Join","") 

corpus1=Corpus(VectorSource(c1$tweet)) # Create corpus

corpus1=tm_map(corpus1,tolower) # Convert to lower-case

corpus1=tm_map(corpus1,function(x) removeWords(x,stopwords())) # Remove stopwords  

dtm <- TermDocumentMatrix(corpus1)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)

col=brewer.pal(8,"Dark2")

#Create word cloud
wordcloud(d$word,freq = d$freq, min.freq=10, scale=c(5,.3),rot.per = 0.35,
          max.word=100, random.order=F,colors=col, use.r.layout=FALSE)

#Create bar plot
barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")

